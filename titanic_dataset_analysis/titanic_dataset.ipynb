{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MrBC530gmg5"
      },
      "source": [
        "### **Problem** \n",
        "**Our goal is to create a predictive model that can answer the following question:**\n",
        "\n",
        "**What kind of people had a better chance of surviving?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhBfwzBgioTD"
      },
      "source": [
        "**Data about passengers:**\n",
        "*   Name\n",
        "*   Age\n",
        "*   Gender.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEH8iZqi-sk"
      },
      "source": [
        "## Install and Import Libraries\n",
        "Let's install PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "oj1HhvIOY5Yz"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDp80mG9jmfU"
      },
      "source": [
        "## Build Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "ttzML9fpjE5a"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Sructured Streaming\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiqECDzLj1Mg"
      },
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn-hxNggkTqV"
      },
      "source": [
        "You have two datasets: \n",
        "* Train  \n",
        "* Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8-A8M7QmKDJ"
      },
      "source": [
        "Read two datasets: \n",
        "* Train\n",
        "* Test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Mx2qAccBk15y"
      },
      "outputs": [],
      "source": [
        "SCHEMA = \"\"\"\n",
        "PassengerId INT, Survived INT, Pclass INT, \n",
        "Name STRING, Sex STRING, Age INT, \n",
        "SibSp INT, Parch INT, Ticket STRING, \n",
        "Fare FLOAT, Cabin STRING, Embarked STRING\"\"\"\n",
        "df_train = spark.read.format('csv')\\\n",
        "    .schema(SCHEMA)\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .load(\"train.csv\")\n",
        "\n",
        "df_test = spark.read.format('csv')\\\n",
        "    .schema(SCHEMA)\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .load(\"train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj2ANTnWmSCq"
      },
      "source": [
        "Let's work with train dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5mWJR30lNs5"
      },
      "source": [
        "**Confirm if this is a dataframe or not:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "tEYTePrzk9yl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvLJElPrlT4i"
      },
      "source": [
        "**Show 5 rows.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "jYwhqvV8lnO0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_train.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIYVxRXlnnw"
      },
      "source": [
        "**Display schema for the dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "pcvERiICl1Ep"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: float (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_train.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmE3Wd80l1S6"
      },
      "source": [
        "**Statistical summary:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "cNY0SItol5Mo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[summary: string, PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string]"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiFaIEQTl70_"
      },
      "source": [
        "## EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSNPOnP8mw2Q"
      },
      "source": [
        "**Display count for the train dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "zrtpG11Fl9HM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "891"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.select('*').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_6nnTfxm9_x"
      },
      "source": [
        "\n",
        "**How many people survived, and how many didn't survive?** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "QDoqPwyomYxA"
      },
      "outputs": [],
      "source": [
        "n_survived = df_train.filter(col('Survived') == 1).count()\n",
        "n_died = df_train.filter(col('Survived') == 0).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "0XHAK8ceoCMU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of those who survived: 342\n",
            "Count of those who didn't survive: 549\n"
          ]
        }
      ],
      "source": [
        "print(\"Count of those who survived:\", n_survived)\n",
        "print(\"Count of those who didn't survive:\", n_died)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "3uiaN29PoQnf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratio of those who survived: 0.3838383838383838\n",
            "Ratio of those who didn't survive: 0.6161616161616161\n"
          ]
        }
      ],
      "source": [
        "print(\"Ratio of those who survived:\", n_survived / (n_survived + n_died))\n",
        "print(\"Ratio of those who didn't survive:\", n_died / (n_survived + n_died))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "XllkDlo3ongJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of males:  577\n",
            "Number of females:  314\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of males: \", df_train.filter(col('Sex') == 'male').count())\n",
        "print(\"Number of females: \", df_train.filter(col('Sex') == 'female').count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHFaJ15zqtEV"
      },
      "source": [
        "**1. What is the average number of survivors of each gender?**\n",
        "\n",
        "**2. What is the number of survivors of each gender?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "NUikH7MUqdKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of survived for each gender\n",
            "+------+-------------+\n",
            "|   Sex|sum(Survived)|\n",
            "+------+-------------+\n",
            "|female|          233|\n",
            "|  male|          109|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Number of survived for each gender')\n",
        "survived_by_gender = df_train.groupBy('Sex').agg(sum('Survived'))\n",
        "survived_by_gender.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average number of survivors for both genders\n",
            "+------------------+\n",
            "|avg(sum(Survived))|\n",
            "+------------------+\n",
            "|             171.0|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Average number of survivors for both genders\")\n",
        "survived_by_gender.agg(avg('sum(Survived)')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCEdYNdArtRN"
      },
      "source": [
        "**Create temporary view PySpark:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "YjlK6HDUqsI5"
      },
      "outputs": [],
      "source": [
        "df_train.createOrReplaceTempView('table')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXNePifnshHr"
      },
      "source": [
        "**How many people survived, and how many didn't survive? By SQL:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "0HxfPRTMslqk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---------------+\n",
            "|Survived|count(Survived)|\n",
            "+--------+---------------+\n",
            "|       1|            342|\n",
            "|       0|            549|\n",
            "+--------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT Survived, count(Survived)\n",
        "    FROM table\n",
        "    GROUP BY Survived;\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVCdY6EasFWV"
      },
      "source": [
        "**Display the number of survivors from each gender as a ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "7xQc3pUUr3HF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|   Sex|Ratio_of_Survivors|\n",
            "+------+------------------+\n",
            "|female|0.2615039281705948|\n",
            "|  male| 0.122334455667789|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT Sex, \n",
        "        sum(Survived)  / (SELECT count(Survived) FROM table) AS Ratio_of_Survivors\n",
        "    FROM table\n",
        "    GROUP BY Sex;\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6QXc5V8uu3Y"
      },
      "source": [
        "**Display a ratio for \"p-class\":**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "Mscs2mDFdFsD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------------+\n",
            "|Pclass|     Survival_Ratio|\n",
            "+------+-------------------+\n",
            "|     1| 0.6296296296296297|\n",
            "|     3|0.24236252545824846|\n",
            "|     2|0.47282608695652173|\n",
            "+------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT Pclass, sum(Survived) / count(Pclass) AS Survival_Ratio\n",
        "    FROM table\n",
        "    GROUP BY Pclass;\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ctM9t8atxJl"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "8Nm8S1K0r4uY"
      },
      "outputs": [],
      "source": [
        "combined = df_train.union(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI7AD8FLz3iO"
      },
      "source": [
        "**Display count:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "s_WERAL8wvJa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1782"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R4Miuy0z_uP"
      },
      "source": [
        "**Define the number of null values in each column?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "0LMOalKBxhpD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|          0|       0|     0|   0|  0|404|    0|    0|     0|   0| 1374|       4|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "combined.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined.columns)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBX8cJ000aqe"
      },
      "source": [
        "**Create Dataframe for null values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ITmyUelNxjJM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|          0|       0|     0|   0|  0|404|    0|    0|     0|   0| 1374|       4|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_na = combined.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in combined.columns))\n",
        "df_na.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuKrOi5a0-Ma"
      },
      "source": [
        "## Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVQlr9vDy7Y4"
      },
      "source": [
        "**Create Temporary view PySpark:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "xs3yeXhGI8rv"
      },
      "outputs": [],
      "source": [
        "combined.createOrReplaceTempView('full_table')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txa8NZIO1JaP"
      },
      "source": [
        "**Can you show the \"name\" column from your temporary table?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "m7yXqJoJy35k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                Name|\n",
            "+--------------------+\n",
            "|Braund, Mr. Owen ...|\n",
            "|Cumings, Mrs. Joh...|\n",
            "|Heikkinen, Miss. ...|\n",
            "|Futrelle, Mrs. Ja...|\n",
            "|Allen, Mr. Willia...|\n",
            "|    Moran, Mr. James|\n",
            "|McCarthy, Mr. Tim...|\n",
            "|Palsson, Master. ...|\n",
            "|Johnson, Mrs. Osc...|\n",
            "|Nasser, Mrs. Nich...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"SELECT Name FROM full_table LIMIT 10;\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "0kx6OcB-2BBT"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "combined = combined.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
        "combined.createOrReplaceTempView('combined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZeUWS12r59"
      },
      "source": [
        "**Display \"Title\" column and count \"Title\" column:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "hGkFMtlp1FAI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+\n",
            "|   Title|Counts|\n",
            "+--------+------+\n",
            "|Countess|     2|\n",
            "|Jonkheer|     2|\n",
            "|     Mme|     2|\n",
            "|    Lady|     2|\n",
            "|     Don|     2|\n",
            "|     Sir|     2|\n",
            "|      Ms|     2|\n",
            "|    Capt|     2|\n",
            "|     Col|     4|\n",
            "|    Mlle|     4|\n",
            "|   Major|     4|\n",
            "|     Rev|    12|\n",
            "|      Dr|    14|\n",
            "|  Master|    80|\n",
            "|     Mrs|   250|\n",
            "|    Miss|   364|\n",
            "|      Mr|  1034|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "count_by_name = spark.sql(\"SELECT Title, count(Title) AS Counts FROM combined GROUP BY Title ORDER BY Counts ASC;\")\n",
        "count_by_name.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBQDKYu4JOa"
      },
      "source": [
        "**We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so create Dictionary and set the value to \"rare\".**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "rjnx5l5r2Qaf"
      },
      "outputs": [],
      "source": [
        "titles_list = count_by_name.select('Title').limit(13).collect()\n",
        "titles_map = {c[0] : 'rare' for c in titles_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "HdDbWuDl7Pf4"
      },
      "outputs": [],
      "source": [
        "def impute_title(title):\n",
        "    return titles_map[title]# Title_map is your dictionary. please change this name with your dictionary name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5EQVIhK7a9R"
      },
      "source": [
        "**Apply the function on \"Title\" column using UDF:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "rBAiIOn77XFa"
      },
      "outputs": [],
      "source": [
        "def impute_wrapper(title):\n",
        "    \"\"\"Function checks if title is one of the rare ones\"\"\"\n",
        "    if title in titles_map:\n",
        "        return(impute_title(title))\n",
        "    else:\n",
        "        return(title)\n",
        "impute_title_udf = udf(lambda t: impute_wrapper(t))\n",
        "\n",
        "# Apply function on `combined` dataframe\n",
        "combined = combined.withColumn('Title', impute_title_udf(col('Title')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8ewllf7kiV"
      },
      "source": [
        "**Display \"Title\" from table and group by \"Title\" column:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "J9sjQb084GU6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "| Title|Counts|\n",
            "+------+------+\n",
            "|  rare|    54|\n",
            "|Master|    80|\n",
            "|   Mrs|   250|\n",
            "|  Miss|   364|\n",
            "|    Mr|  1034|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "combined.createOrReplaceTempView('combined')\n",
        "spark.sql(\"SELECT Title, count(Title) AS Counts FROM combined GROUP BY Title ORDER BY Counts ASC;\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H45QNLj9vJp"
      },
      "source": [
        "## **Preprocessing Age**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "eXYSVzvl4z63"
      },
      "outputs": [],
      "source": [
        "age_avg = combined.select(mean(col('age'))).collect()[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLPivde8_GI-"
      },
      "source": [
        "**Fill missing with \"age\" mean:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "lBgW8aFD90PA"
      },
      "outputs": [],
      "source": [
        "combined = combined.fillna(age_avg, subset=['age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsnUz-m_P95"
      },
      "source": [
        "## **Preprocessing Embarked**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbbamcXMSYP"
      },
      "source": [
        "**Select \"Embarked\" column, count them, order by count Desc, and save in grouped_Embarked variable:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "v-lRu5vc_FW7"
      },
      "outputs": [],
      "source": [
        "grouped_Embarked = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT Embarked, count(Embarked) AS Counts \n",
        "    FROM combined \n",
        "    GROUP BY Embarked\n",
        "    ORDER BY Counts DESC;\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "jSFNDTNg_erb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+\n",
            "|Embarked|Counts|\n",
            "+--------+------+\n",
            "|       S|  1288|\n",
            "|       C|   336|\n",
            "|       Q|   154|\n",
            "|    null|     0|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "grouped_Embarked.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQWYgKBMrbp"
      },
      "source": [
        "**Get max of groupped_Embarked:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "ZLYj4F7E_iqb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('S', 1288)"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_Embarked.limit(1).collect()[0][:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vhoEs8N2w_"
      },
      "source": [
        "**Fill missing values with max 'S' of grouped_Embarked:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "LdzQCRud_mAa"
      },
      "outputs": [],
      "source": [
        "combined = combined.fillna('S', subset=['Embarked'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEcdV5Vb_qR_"
      },
      "source": [
        "## **Preprocessing Cabin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BQzPs7tqhpA"
      },
      "source": [
        "**Replace \"cabin\" column with first char from the string:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "4b6L5pK0_nQz"
      },
      "outputs": [],
      "source": [
        "def first_letter(cabin):\n",
        "    if cabin != None:\n",
        "        return(cabin[0])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "first_letter_udf = udf(lambda c: first_letter(c))\n",
        "combined = combined.withColumn('cabin', first_letter_udf(col('cabin')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H8XshnYj4k2"
      },
      "source": [
        "**Show the result:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "gJUQwnG1Oj2U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|cabin|\n",
            "+-----+\n",
            "| null|\n",
            "|    C|\n",
            "| null|\n",
            "|    C|\n",
            "| null|\n",
            "| null|\n",
            "|    E|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "|    G|\n",
            "|    C|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "| null|\n",
            "+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "combined.select(col('cabin')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSDsWsUj9Im"
      },
      "source": [
        "**Create the temporary view:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "MR7CXTY7_tMJ"
      },
      "outputs": [],
      "source": [
        "combined.createOrReplaceTempView('combined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7lfQFkrLlN"
      },
      "source": [
        "**Select \"Cabin\" column, count \"Cabin\" column, Group by \"Cabin\" column, Order By count DESC**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "A0tZG_mvrKXv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+\n",
            "|cabin|Counts|\n",
            "+-----+------+\n",
            "|    C|   118|\n",
            "|    B|    94|\n",
            "|    D|    66|\n",
            "|    E|    64|\n",
            "|    A|    30|\n",
            "|    F|    26|\n",
            "|    G|     8|\n",
            "|    T|     2|\n",
            "| null|     0|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT cabin,\n",
        "        count(cabin) AS Counts\n",
        "    FROM combined\n",
        "    GROUP BY cabin\n",
        "    ORDER BY Counts DESC; \n",
        "    \"\"\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR6j0LOsB4y"
      },
      "source": [
        "**Fill missing values with \"U\":**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "mwq5CHEz_up_"
      },
      "outputs": [],
      "source": [
        "combined = combined.fillna('U', subset=['cabin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "odhuI2EHKuCm"
      },
      "outputs": [],
      "source": [
        "categorical = [col for (col, dtype) in combined.dtypes if dtype == 'string']\n",
        "numerical = [col for (col, dtype) in combined.dtypes if dtype != 'string']\n",
        "\n",
        "index_cols = [x + \"_Index\" for x in categorical]\n",
        "ohe_cols = [x + \"_OHE\" for x in categorical]\n",
        "\n",
        "string_indexer = StringIndexer(inputCols=categorical, outputCols=index_cols, handleInvalid='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "tiAjDEy1LBhz"
      },
      "outputs": [],
      "source": [
        "ohe_encoder = OneHotEncoder(inputCols=index_cols, outputCols=ohe_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "BuytKk0hLE6p"
      },
      "outputs": [],
      "source": [
        "vec_assembler = VectorAssembler(inputCols=ohe_cols + numerical, outputCol='features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU8DeZfh7JIo"
      },
      "source": [
        "**Use randomSplit function and split data to x_train, and X_test with 80% and 20% Consecutive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "8C11xf1iAzKp"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = combined.randomSplit([.8, .2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJQvmFai72O7"
      },
      "source": [
        "**Build RandomForestClassifier model and use pipeline to fit and transform then display \"prediction, Survived, features\" columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "YnpmZqlTLPGq"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(labelCol=\"Survived\", seed=42, featuresCol='features')\n",
        "pipeline = Pipeline(stages=[string_indexer, ohe_encoder, vec_assembler, model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSXEI8-r8bKY"
      },
      "source": [
        "**Use MulticlassClassificationEvaluator and set the \"labelCol\" to \"Survived\",  \"predictionCol\" to \"prediction\", \"metricName\" to \"accuracy\"** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "Rl0UAKCaBDO-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21/11/04 22:01:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|            features|prediction|\n",
            "+--------------------+----------+\n",
            "|(1540,[700,1516,1...|       1.0|\n",
            "|(1540,[715,915,15...|       1.0|\n",
            "|(1540,[621,856,85...|       0.0|\n",
            "|(1540,[746,1404,1...|       1.0|\n",
            "|(1540,[821,856,13...|       0.0|\n",
            "|(1540,[837,856,14...|       0.0|\n",
            "|(1540,[735,856,14...|       0.0|\n",
            "|(1540,[775,1372,1...|       1.0|\n",
            "|(1540,[624,956,15...|       0.0|\n",
            "|(1540,[771,856,14...|       0.0|\n",
            "|(1540,[695,856,96...|       0.0|\n",
            "|(1540,[720,856,14...|       0.0|\n",
            "|(1540,[765,856,14...|       0.0|\n",
            "|(1540,[788,1450,1...|       0.0|\n",
            "|(1540,[631,856,14...|       0.0|\n",
            "|(1540,[662,856,14...|       0.0|\n",
            "|(1540,[842,856,93...|       0.0|\n",
            "|(1540,[633,856,97...|       0.0|\n",
            "|(1540,[673,856,14...|       0.0|\n",
            "|(1540,[754,856,14...|       0.0|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline_model = pipeline.fit(X_train)\n",
        "df_pred = pipeline_model.transform(X_test)\n",
        "df_pred.select('features', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8984375"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='Survived', metricName='accuracy')\n",
        "evaluator.evaluate(df_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practical_work.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b05b6b0120ed31a8d2080538dac8d164c5f3cf1de022bc2bdb01721ec1cdba3b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
